{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\n!pip install torchsummary\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchsummary import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T01:41:35.786390Z","iopub.execute_input":"2024-04-22T01:41:35.787070Z","iopub.status.idle":"2024-04-22T01:41:48.315266Z","shell.execute_reply.started":"2024-04-22T01:41:35.787036Z","shell.execute_reply":"2024-04-22T01:41:48.314123Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:41:48.317729Z","iopub.execute_input":"2024-04-22T01:41:48.318107Z","iopub.status.idle":"2024-04-22T01:41:48.376213Z","shell.execute_reply.started":"2024-04-22T01:41:48.318072Z","shell.execute_reply":"2024-04-22T01:41:48.375195Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"# minibatch size\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:41:48.377691Z","iopub.execute_input":"2024-04-22T01:41:48.378370Z","iopub.status.idle":"2024-04-22T01:41:48.390058Z","shell.execute_reply.started":"2024-04-22T01:41:48.378337Z","shell.execute_reply":"2024-04-22T01:41:48.389056Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# PreProcessing","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\n\n# Define resize operation\nresize = transforms.Resize((98, 98))\n\n# Define transformations for both training and testing\ntransformations = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n    resize,\n    transforms.RandomHorizontalFlip(),  # Data augmentation\n    transforms.RandomRotation(10),  # Data augmentation\n    transforms.RandomResizedCrop((98, 98), scale=(0.8, 1.2), interpolation=Image.BILINEAR),  # Random crop\n    #transforms.RandomApply([transforms.RandomErasing()], p=0.5),  # Random erasing\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),  # Normalizing for grayscale image\n])\n\n# Use the same transformations for both training and testing datasets\ntrainTransforms = transformations\ntestTransforms = transformations","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:41:48.391484Z","iopub.execute_input":"2024-04-22T01:41:48.392186Z","iopub.status.idle":"2024-04-22T01:41:48.404140Z","shell.execute_reply.started":"2024-04-22T01:41:48.392153Z","shell.execute_reply":"2024-04-22T01:41:48.403277Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import random_split\n\n\n\n# Load the dataset from the image folder\ndataset = datasets.ImageFolder(root='/kaggle/input/fer2013/train', transform=trainTransforms)\n\n# Calculate the sizes for train and validation sets\ntrain_size = int(0.7 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Split the dataset\ntrain_data, validation_data = random_split(dataset, [train_size, val_size])\n# Setup the batch size hyperparameter\nBATCH_SIZE = batch_size\ntest_data = ImageFolder('/kaggle/input/fer2013/test', transform=testTransforms)\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\nvalidation_dataloader = DataLoader(validation_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\ntest_dataloader = DataLoader(test_data,\n                             batch_size=BATCH_SIZE,\n                             shuffle=False)\n\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\")\nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of Validation dataloader: {len(validation_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:41:48.406910Z","iopub.execute_input":"2024-04-22T01:41:48.407480Z","iopub.status.idle":"2024-04-22T01:42:21.163682Z","shell.execute_reply.started":"2024-04-22T01:41:48.407456Z","shell.execute_reply":"2024-04-22T01:42:21.162688Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7ad00023bf70>, <torch.utils.data.dataloader.DataLoader object at 0x7ad0b7533250>)\nLength of train dataloader: 1256 batches of 16\nLength of Validation dataloader: 539 batches of 16\nLength of test dataloader: 449 batches of 16\n","output_type":"stream"}]},{"cell_type":"code","source":"nb_train_samples = 5024 * 4\nnb_validation_samples = 2154 * 4\nnb_test_samples = 1795 * 4","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:42:21.165054Z","iopub.execute_input":"2024-04-22T01:42:21.165371Z","iopub.status.idle":"2024-04-22T01:42:21.170385Z","shell.execute_reply.started":"2024-04-22T01:42:21.165345Z","shell.execute_reply":"2024-04-22T01:42:21.169298Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Defining the neural network\nclass FIVE_CNN(nn.Module):\n    def __init__(self):\n        super(FIVE_CNN, self).__init__()\n        \n        # Convolutional blocks: Conv layer => BatchNorm => ReLU\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block4 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Adaptive average pooling\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n         # Fully connected layers\n        self.fc_layers = nn.Sequential(\n            nn.Linear(1024, 4096),  # Adjust the linear layer size based on your conv output\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 7),\n        )\n        \n  \n\n    def forward(self, x):\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = self.conv_block4(x)\n        x = self.conv_block5(x)\n        x = self.adaptive_avg_pool(x)\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = self.fc_layers(x)\n        return x\n\n# Instantiating the model\nmodel = FIVE_CNN()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:42:21.171568Z","iopub.execute_input":"2024-04-22T01:42:21.171844Z","iopub.status.idle":"2024-04-22T01:42:21.501843Z","shell.execute_reply.started":"2024-04-22T01:42:21.171821Z","shell.execute_reply":"2024-04-22T01:42:21.500965Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"CROSS ENTROPY LOSS","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Criterion and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n\n# Scheduler\nscheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.75, patience=5, verbose=True)\n\nnum_epochs = 80\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n\n    for inputs, labels in train_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        _, preds = torch.max(outputs, 1)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n    epoch_loss = running_loss / len(train_dataloader.dataset)\n    epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n    print(f'Epoch {epoch}/{num_epochs - 1} - Training loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_running_corrects = 0\n    with torch.no_grad():\n        for inputs, labels in validation_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            val_running_loss += loss.item() * inputs.size(0)\n            val_running_corrects += torch.sum(preds == labels.data)\n\n    val_epoch_loss = val_running_loss / len(validation_dataloader.dataset)\n    val_epoch_acc = val_running_corrects.double() / len(validation_dataloader.dataset)\n    print(f'Epoch {epoch}/{num_epochs - 1} - Validation loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}')\n\n    # Adjust learning rate based on validation accuracy\n    scheduler.step(val_epoch_acc)\n\n# Save the model after training\ntorch.save(model.state_dict(), 'vgg_cnn_fer_final_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:48:03.693922Z","iopub.execute_input":"2024-04-22T03:48:03.694543Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0/79 - Training loss: 0.7391, Accuracy: 0.7266\nEpoch 0/79 - Validation loss: 1.2091, Accuracy: 0.5915\nEpoch 1/79 - Training loss: 0.7385, Accuracy: 0.7236\nEpoch 1/79 - Validation loss: 1.1191, Accuracy: 0.6130\nEpoch 2/79 - Training loss: 0.7247, Accuracy: 0.7298\nEpoch 2/79 - Validation loss: 1.1075, Accuracy: 0.6075\nEpoch 3/79 - Training loss: 0.7182, Accuracy: 0.7345\nEpoch 3/79 - Validation loss: 1.2054, Accuracy: 0.6022\nEpoch 4/79 - Training loss: 0.7125, Accuracy: 0.7334\nEpoch 4/79 - Validation loss: 1.2302, Accuracy: 0.5983\nEpoch 5/79 - Training loss: 0.6963, Accuracy: 0.7402\nEpoch 5/79 - Validation loss: 1.1290, Accuracy: 0.6230\nEpoch 6/79 - Training loss: 0.6849, Accuracy: 0.7454\nEpoch 6/79 - Validation loss: 1.0861, Accuracy: 0.6206\nEpoch 7/79 - Training loss: 0.6802, Accuracy: 0.7454\nEpoch 7/79 - Validation loss: 1.1346, Accuracy: 0.6034\nEpoch 8/79 - Training loss: 0.6732, Accuracy: 0.7529\nEpoch 8/79 - Validation loss: 1.3193, Accuracy: 0.5945\nEpoch 9/79 - Training loss: 0.6525, Accuracy: 0.7602\nEpoch 9/79 - Validation loss: 1.1597, Accuracy: 0.6040\nEpoch 10/79 - Training loss: 0.6546, Accuracy: 0.7560\nEpoch 10/79 - Validation loss: 1.1673, Accuracy: 0.5984\nEpoch 11/79 - Training loss: 0.6437, Accuracy: 0.7621\nEpoch 11/79 - Validation loss: 1.1850, Accuracy: 0.6156\nEpoch 00012: reducing learning rate of group 0 to 7.5000e-04.\nEpoch 12/79 - Training loss: 0.5865, Accuracy: 0.7825\nEpoch 12/79 - Validation loss: 1.1950, Accuracy: 0.6228\nEpoch 13/79 - Training loss: 0.5759, Accuracy: 0.7867\nEpoch 13/79 - Validation loss: 1.2247, Accuracy: 0.6236\nEpoch 14/79 - Training loss: 0.5670, Accuracy: 0.7886\nEpoch 14/79 - Validation loss: 1.3865, Accuracy: 0.5794\nEpoch 15/79 - Training loss: 0.5551, Accuracy: 0.7949\nEpoch 15/79 - Validation loss: 1.2681, Accuracy: 0.6084\nEpoch 16/79 - Training loss: 0.5545, Accuracy: 0.7949\nEpoch 16/79 - Validation loss: 1.1827, Accuracy: 0.6180\nEpoch 17/79 - Training loss: 0.5465, Accuracy: 0.8000\nEpoch 17/79 - Validation loss: 1.2765, Accuracy: 0.5890\nEpoch 18/79 - Training loss: 0.5426, Accuracy: 0.8016\nEpoch 18/79 - Validation loss: 1.3447, Accuracy: 0.5867\nEpoch 19/79 - Training loss: 0.5286, Accuracy: 0.8073\nEpoch 19/79 - Validation loss: 1.2037, Accuracy: 0.6310\nEpoch 20/79 - Training loss: 0.5269, Accuracy: 0.8053\nEpoch 20/79 - Validation loss: 1.2521, Accuracy: 0.6036\nEpoch 21/79 - Training loss: 0.5130, Accuracy: 0.8102\nEpoch 21/79 - Validation loss: 1.2143, Accuracy: 0.6229\nEpoch 22/79 - Training loss: 0.5082, Accuracy: 0.8133\nEpoch 22/79 - Validation loss: 1.3181, Accuracy: 0.5955\nEpoch 23/79 - Training loss: 0.4996, Accuracy: 0.8143\nEpoch 23/79 - Validation loss: 1.2400, Accuracy: 0.6051\nEpoch 24/79 - Training loss: 0.5006, Accuracy: 0.8160\nEpoch 24/79 - Validation loss: 1.5141, Accuracy: 0.5616\nEpoch 25/79 - Training loss: 0.4862, Accuracy: 0.8211\nEpoch 25/79 - Validation loss: 1.2976, Accuracy: 0.6120\nEpoch 00026: reducing learning rate of group 0 to 5.6250e-04.\nEpoch 26/79 - Training loss: 0.4369, Accuracy: 0.8380\nEpoch 26/79 - Validation loss: 1.2483, Accuracy: 0.6361\nEpoch 27/79 - Training loss: 0.4310, Accuracy: 0.8395\nEpoch 27/79 - Validation loss: 1.5022, Accuracy: 0.5802\nEpoch 28/79 - Training loss: 0.4307, Accuracy: 0.8445\nEpoch 28/79 - Validation loss: 1.3191, Accuracy: 0.6119\nEpoch 29/79 - Training loss: 0.4131, Accuracy: 0.8477\nEpoch 29/79 - Validation loss: 1.4088, Accuracy: 0.5946\nEpoch 30/79 - Training loss: 0.4048, Accuracy: 0.8523\nEpoch 30/79 - Validation loss: 1.3385, Accuracy: 0.6256\nEpoch 31/79 - Training loss: 0.4011, Accuracy: 0.8535\nEpoch 31/79 - Validation loss: 1.3148, Accuracy: 0.6127\nEpoch 32/79 - Training loss: 0.3955, Accuracy: 0.8541\nEpoch 32/79 - Validation loss: 1.4264, Accuracy: 0.5998\nEpoch 00033: reducing learning rate of group 0 to 4.2188e-04.\nEpoch 33/79 - Training loss: 0.3596, Accuracy: 0.8691\nEpoch 33/79 - Validation loss: 1.2921, Accuracy: 0.6280\nEpoch 34/79 - Training loss: 0.3441, Accuracy: 0.8747\nEpoch 34/79 - Validation loss: 1.3671, Accuracy: 0.6158\nEpoch 35/79 - Training loss: 0.3396, Accuracy: 0.8754\nEpoch 35/79 - Validation loss: 1.4388, Accuracy: 0.6257\nEpoch 36/79 - Training loss: 0.3320, Accuracy: 0.8791\nEpoch 36/79 - Validation loss: 1.3420, Accuracy: 0.6378\nEpoch 37/79 - Training loss: 0.3270, Accuracy: 0.8796\nEpoch 37/79 - Validation loss: 1.3784, Accuracy: 0.6457\nEpoch 38/79 - Training loss: 0.3340, Accuracy: 0.8788\nEpoch 38/79 - Validation loss: 1.4941, Accuracy: 0.6151\nEpoch 39/79 - Training loss: 0.3121, Accuracy: 0.8854\nEpoch 39/79 - Validation loss: 1.6023, Accuracy: 0.5945\nEpoch 40/79 - Training loss: 0.3219, Accuracy: 0.8839\nEpoch 40/79 - Validation loss: 1.4524, Accuracy: 0.6387\nEpoch 41/79 - Training loss: 0.3152, Accuracy: 0.8869\nEpoch 41/79 - Validation loss: 1.4520, Accuracy: 0.6076\nEpoch 42/79 - Training loss: 0.3172, Accuracy: 0.8867\nEpoch 42/79 - Validation loss: 1.3799, Accuracy: 0.6248\nEpoch 43/79 - Training loss: 0.3089, Accuracy: 0.8895\nEpoch 43/79 - Validation loss: 1.3630, Accuracy: 0.6402\nEpoch 00044: reducing learning rate of group 0 to 3.1641e-04.\nEpoch 44/79 - Training loss: 0.2817, Accuracy: 0.8994\nEpoch 44/79 - Validation loss: 1.4512, Accuracy: 0.6390\nEpoch 45/79 - Training loss: 0.2670, Accuracy: 0.9040\nEpoch 45/79 - Validation loss: 1.5236, Accuracy: 0.6231\nEpoch 46/79 - Training loss: 0.2617, Accuracy: 0.9077\nEpoch 46/79 - Validation loss: 1.4617, Accuracy: 0.6379\nEpoch 47/79 - Training loss: 0.2581, Accuracy: 0.9073\nEpoch 47/79 - Validation loss: 1.4332, Accuracy: 0.6445\nEpoch 48/79 - Training loss: 0.2536, Accuracy: 0.9097\nEpoch 48/79 - Validation loss: 1.4937, Accuracy: 0.6395\nEpoch 49/79 - Training loss: 0.2578, Accuracy: 0.9070\nEpoch 49/79 - Validation loss: 1.4477, Accuracy: 0.6422\nEpoch 00050: reducing learning rate of group 0 to 2.3730e-04.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\ntest_corrects = 0\n\n# You should define nb_test_samples before this block\n# It should be the total number of samples in the test set\nnb_test_samples = len(test_dataloader.dataset)\n\nwith torch.no_grad():\n    for inputs, labels in test_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item() * inputs.size(0)\n\n        _, preds = torch.max(outputs, 1)\n        test_corrects += torch.sum(preds == labels.data).item()\n\n# Calculate the average loss and accuracy\ntest_epoch_loss = test_loss / nb_test_samples\ntest_epoch_accuracy = test_corrects / nb_test_samples\ntest_accuracy_percentage = test_epoch_accuracy * 100\n\nprint(f'Test loss: {test_epoch_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy_percentage:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T03:12:27.842315Z","iopub.execute_input":"2024-04-22T03:12:27.842726Z","iopub.status.idle":"2024-04-22T03:13:02.014340Z","shell.execute_reply.started":"2024-04-22T03:12:27.842697Z","shell.execute_reply":"2024-04-22T03:13:02.013356Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Test loss: 1.1320\nTest Accuracy: 62.93%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}