{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\n!pip install torchsummary\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2024-05-04T22:50:30.978578Z","iopub.execute_input":"2024-05-04T22:50:30.978978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# minibatch size\nbatch_size = 16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\n\n# Define resize operation\nresize = transforms.Resize((98, 98))\n\n# Define transformations for both training and testing\ntransformations = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n    resize,\n    transforms.RandomHorizontalFlip(),  # Data augmentation\n    transforms.RandomRotation(10),  # Data augmentation\n    transforms.RandomResizedCrop((98, 98), scale=(0.8, 1.2), interpolation=Image.BILINEAR),  # Random crop\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),  # Normalizing for grayscale image\n])\n\n# Use the same transformations for both training and testing datasets\ntrainTransforms = transformations\ntestTransforms = transformations\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import random_split\n\n\n# Load the dataset from the image folder\ndataset = datasets.ImageFolder(root='/kaggle/input/fer2013/train', transform=trainTransforms)\n\n# Calculate the sizes for train and validation sets\ntrain_size = int(0.7 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Split the dataset\ntrain_data, validation_data = random_split(dataset, [train_size, val_size])\n# Setup the batch size hyperparameter\nBATCH_SIZE = batch_size\ntest_data = ImageFolder('/kaggle/input/fer2013/test', transform=testTransforms)\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\nvalidation_dataloader = DataLoader(validation_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\ntest_dataloader = DataLoader(test_data,\n                             batch_size=BATCH_SIZE,\n                             shuffle=False)\n\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\")\nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of Validation dataloader: {len(validation_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_train_samples = 1256 * 16\nnb_validation_samples = 539 * 16\nnb_test_samples = 449 * 16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Defining the neural network\nclass Designed_Model(nn.Module):\n    def __init__(self):\n        super(Designed_Model, self).__init__()\n        \n        # Convolutional blocks: Conv layer => BatchNorm => ReLU\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block4 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.conv_block5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        \n        # Adaptive average pooling\n        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        # Fully connected layers\n        self.fc_layers = nn.Sequential(\n            nn.Linear(1024, 4096),  # Adjust the linear layer size based on your conv output\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 7),\n        ) \n        \n  \n\n    def forward(self, x):\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = self.conv_block4(x)\n        x = self.conv_block5(x)\n        x = self.adaptive_avg_pool(x)\n        x = x.view(x.size(0), -1)  \n        x = self.fc_layers(x)\n        return x\n\n# Instantiating the model\nmodel = Designed_Model()\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport torch.nn.functional as F\n\ncriterion = nn.CrossEntropyLoss()\n \n# Adam Optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n# Number of epochs\nnum_epochs = 50\n\n\n# Move model to GPU if available\nmodel = model.to(device)\n\n# Training Loop\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n\n    for inputs, labels in train_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)  # Use KLD loss here\n        _, preds = torch.max(outputs, 1)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n    epoch_loss = running_loss / len(train_dataloader.dataset)\n    epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n    print(f'Epoch {epoch}/{num_epochs - 1} - Training loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_running_corrects = 0\n    \n    with torch.no_grad():\n        for inputs, labels in validation_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)  # Use KLD loss here\n            _, preds = torch.max(outputs, 1)\n            val_running_loss += loss.item() * inputs.size(0)\n            val_running_corrects += torch.sum(preds == labels.data)\n\n    val_epoch_loss = val_running_loss / len(validation_dataloader.dataset)\n    val_epoch_acc = val_running_corrects.double() / len(validation_dataloader.dataset)\n    print(f'Epoch {epoch}/{num_epochs - 1} - Validation loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}')\n\n    \n\n# Save the model after training\ntorch.save(model.state_dict(), 'vggnet_CE_adam.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\ntest_corrects = 0\n\n\nnb_test_samples = len(test_dataloader.dataset)\n\nwith torch.no_grad():\n    for inputs, labels in test_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item() * inputs.size(0)\n\n        _, preds = torch.max(outputs, 1)\n        test_corrects += torch.sum(preds == labels.data).item()\n\n# Calculate the average loss and accuracy\ntest_epoch_loss = test_loss / nb_test_samples\ntest_epoch_accuracy = test_corrects / nb_test_samples\ntest_accuracy_percentage = test_epoch_accuracy * 100\n\nprint(f'Test loss: {test_epoch_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy_percentage:.2f}%')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}