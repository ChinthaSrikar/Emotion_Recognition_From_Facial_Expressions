{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8049663,"sourceType":"datasetVersion","datasetId":4746924},{"sourceId":7918945,"sourceType":"datasetVersion","datasetId":4653255}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\n!pip install torchsummary\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchsummary import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T03:28:07.644469Z","iopub.execute_input":"2024-04-07T03:28:07.645244Z","iopub.status.idle":"2024-04-07T03:28:19.783695Z","shell.execute_reply.started":"2024-04-07T03:28:07.645210Z","shell.execute_reply":"2024-04-07T03:28:19.782455Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:28:31.844937Z","iopub.execute_input":"2024-04-07T03:28:31.845974Z","iopub.status.idle":"2024-04-07T03:28:31.855270Z","shell.execute_reply.started":"2024-04-07T03:28:31.845923Z","shell.execute_reply":"2024-04-07T03:28:31.854176Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"# minibatch size\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:28:33.563392Z","iopub.execute_input":"2024-04-07T03:28:33.564187Z","iopub.status.idle":"2024-04-07T03:28:33.568209Z","shell.execute_reply.started":"2024-04-07T03:28:33.564150Z","shell.execute_reply":"2024-04-07T03:28:33.567239Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# PreProcessing","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\nimport random\n\n# Define resize operation if needed\nresize = transforms.Resize((48, 48))  # Replace with (96, 96) if upsizing is desired\n\n# Define transformations for both training and testing\ntransformations = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n    resize,\n    transforms.RandomHorizontalFlip(),  # Data augmentation\n    transforms.RandomRotation(10),  # Data augmentation\n    transforms.RandomResizedCrop((40, 40), scale=(0.8, 1.2), interpolation=Image.BILINEAR),  # Random crop\n    #transforms.RandomApply([transforms.RandomErasing()], p=0.5),  # Random erasing\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),  # Normalizing for grayscale image\n])\n\n# Use the same transformations for both training and testing datasets\ntrainTransforms = transformations\ntestTransforms = transformations","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:28:35.957451Z","iopub.execute_input":"2024-04-07T03:28:35.958155Z","iopub.status.idle":"2024-04-07T03:28:35.965573Z","shell.execute_reply.started":"2024-04-07T03:28:35.958119Z","shell.execute_reply":"2024-04-07T03:28:35.964516Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import random_split\n\n\n\n\n# Load the dataset from the image folder\ndataset = datasets.ImageFolder(root='/kaggle/input/facial-emotion-detection/Facial emotion/train', transform=trainTransforms)\n\n# Calculate the sizes for train and validation sets\ntrain_size = int(0.7 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Split the dataset\ntrain_data, validation_data = random_split(dataset, [train_size, val_size])\n# Setup the batch size hyperparameter\nBATCH_SIZE = batch_size\ntest_data = ImageFolder('/kaggle/input/facial-emotion-detection/Facial emotion/test', transform=testTransforms)\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\nvalidation_dataloader = DataLoader(validation_data,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n\ntest_dataloader = DataLoader(test_data,\n                             batch_size=BATCH_SIZE,\n                             shuffle=False)\n\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\")\nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of Validation dataloader: {len(validation_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:28:39.171539Z","iopub.execute_input":"2024-04-07T03:28:39.171936Z","iopub.status.idle":"2024-04-07T03:28:48.314987Z","shell.execute_reply.started":"2024-04-07T03:28:39.171906Z","shell.execute_reply":"2024-04-07T03:28:48.314020Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7b2ce963dcc0>, <torch.utils.data.dataloader.DataLoader object at 0x7b2cf29f68c0>)\nLength of train dataloader: 1256 batches of 16\nLength of Validation dataloader: 539 batches of 16\nLength of test dataloader: 449 batches of 16\n","output_type":"stream"}]},{"cell_type":"code","source":"nb_train_samples = 1256 * 16\nnb_validation_samples = 539 * 16\nnb_test_samples = 449 * 16","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:29:18.170137Z","iopub.execute_input":"2024-04-07T03:29:18.170947Z","iopub.status.idle":"2024-04-07T03:29:18.175724Z","shell.execute_reply.started":"2024-04-07T03:29:18.170908Z","shell.execute_reply":"2024-04-07T03:29:18.174666Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# **VGG-11**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass VGGNetFER(nn.Module):\n    def __init__(self, num_classes=7):\n        super(VGGNetFER, self).__init__()\n        # Convolutional blocks\n        self.conv_blocks = nn.Sequential(\n            # Conv Block 1\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Conv Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Conv Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Conv Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        \n        # Fully connected layers\n        self.fc_layers = nn.Sequential(\n            nn.Linear(512 * 3 * 3, 4096),  # Adjust the linear layer size based on your conv output\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.conv_blocks(x)\n        x = x.view(x.size(0), -1)  # Flatten the output of conv layers\n        x = self.fc_layers(x)\n        return x  \n\n# Create the model\nnum_classes = 7  # for FER2013 dataset\nmodel = VGGNetFER(num_classes)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T02:51:33.623547Z","iopub.execute_input":"2024-04-04T02:51:33.623878Z","iopub.status.idle":"2024-04-04T02:51:34.056223Z","shell.execute_reply.started":"2024-04-04T02:51:33.623845Z","shell.execute_reply":"2024-04-04T02:51:34.055372Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RESNET-34**","metadata":{}},{"cell_type":"code","source":"class BlockBuilder(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BlockBuilder, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        identity = self.shortcut(x)\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        x += identity\n        x = torch.relu(x)\n        return x\n\n\nclass ResNet_model(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(ResNet_model, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.residual_layer1 = self._make_residual_layer(block, 64, num_blocks[0], stride=1)\n        self.residual_layer2 = self._make_residual_layer(block, 128, num_blocks[1], stride=2)\n        self.residual_layer3 = self._make_residual_layer(block, 256, num_blocks[2], stride=2)\n        self.residual_layer4 = self._make_residual_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  \n        self.linear = nn.Linear(512*block.expansion,36)\n\n    def _make_residual_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.residual_layer1(x)\n        x = self.residual_layer2(x)\n        x = self.residual_layer3(x)\n        x = self.residual_layer4(x)\n        x = self.avgpool(x) \n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n\ndef ResNet34():\n    return ResNet_model(BlockBuilder, [3, 4, 6, 3])\n\nmodel = ResNet34().to(device)\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:29:21.996793Z","iopub.execute_input":"2024-04-07T03:29:21.997188Z","iopub.status.idle":"2024-04-07T03:29:22.277369Z","shell.execute_reply.started":"2024-04-07T03:29:21.997155Z","shell.execute_reply":"2024-04-07T03:29:22.276407Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"ResNet_model(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (residual_layer1): Sequential(\n    (0): BlockBuilder(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (1): BlockBuilder(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): BlockBuilder(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (residual_layer2): Sequential(\n    (0): BlockBuilder(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BlockBuilder(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): BlockBuilder(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): BlockBuilder(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (residual_layer3): Sequential(\n    (0): BlockBuilder(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BlockBuilder(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): BlockBuilder(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): BlockBuilder(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (4): BlockBuilder(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (5): BlockBuilder(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (residual_layer4): Sequential(\n    (0): BlockBuilder(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BlockBuilder(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): BlockBuilder(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (linear): Linear(in_features=512, out_features=36, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Criterion and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n\n# Scheduler\nscheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.75, patience=5, verbose=True)\n\nnum_epochs = 25\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n\n    for inputs, labels in train_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        _, preds = torch.max(outputs, 1)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n    epoch_loss = running_loss / len(train_dataloader.dataset)\n    epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n    print(f'Epoch {epoch}/{num_epochs - 1} - Training loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_running_corrects = 0\n    with torch.no_grad():\n        for inputs, labels in validation_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            val_running_loss += loss.item() * inputs.size(0)\n            val_running_corrects += torch.sum(preds == labels.data)\n\n    val_epoch_loss = val_running_loss / len(validation_dataloader.dataset)\n    val_epoch_acc = val_running_corrects.double() / len(validation_dataloader.dataset)\n    print(f'Epoch {epoch}/{num_epochs - 1} - Validation loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}')\n\n    # Adjust learning rate based on validation accuracy\n    scheduler.step(val_epoch_acc)\n\n# Save the model after training\n#torch.save(model.state_dict(), 'vggnet_fer.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:29:27.829454Z","iopub.execute_input":"2024-04-07T03:29:27.830396Z","iopub.status.idle":"2024-04-07T04:04:11.578696Z","shell.execute_reply.started":"2024-04-07T03:29:27.830341Z","shell.execute_reply":"2024-04-07T04:04:11.577566Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 0/24 - Training loss: 1.7165, Accuracy: 0.3160\nEpoch 0/24 - Validation loss: 1.5676, Accuracy: 0.3834\nEpoch 1/24 - Training loss: 1.5378, Accuracy: 0.4038\nEpoch 1/24 - Validation loss: 1.4764, Accuracy: 0.4327\nEpoch 2/24 - Training loss: 1.4505, Accuracy: 0.4350\nEpoch 2/24 - Validation loss: 1.3887, Accuracy: 0.4637\nEpoch 3/24 - Training loss: 1.3818, Accuracy: 0.4677\nEpoch 3/24 - Validation loss: 1.3442, Accuracy: 0.4802\nEpoch 4/24 - Training loss: 1.3202, Accuracy: 0.4938\nEpoch 4/24 - Validation loss: 1.3018, Accuracy: 0.4999\nEpoch 5/24 - Training loss: 1.2724, Accuracy: 0.5135\nEpoch 5/24 - Validation loss: 1.2806, Accuracy: 0.5138\nEpoch 6/24 - Training loss: 1.2357, Accuracy: 0.5307\nEpoch 6/24 - Validation loss: 1.2619, Accuracy: 0.5169\nEpoch 7/24 - Training loss: 1.2013, Accuracy: 0.5459\nEpoch 7/24 - Validation loss: 1.2403, Accuracy: 0.5313\nEpoch 8/24 - Training loss: 1.1631, Accuracy: 0.5565\nEpoch 8/24 - Validation loss: 1.2043, Accuracy: 0.5457\nEpoch 9/24 - Training loss: 1.1361, Accuracy: 0.5713\nEpoch 9/24 - Validation loss: 1.1892, Accuracy: 0.5487\nEpoch 10/24 - Training loss: 1.1089, Accuracy: 0.5811\nEpoch 10/24 - Validation loss: 1.2140, Accuracy: 0.5565\nEpoch 11/24 - Training loss: 1.0810, Accuracy: 0.5915\nEpoch 11/24 - Validation loss: 1.1987, Accuracy: 0.5523\nEpoch 12/24 - Training loss: 1.0592, Accuracy: 0.5961\nEpoch 12/24 - Validation loss: 1.1617, Accuracy: 0.5617\nEpoch 13/24 - Training loss: 1.0339, Accuracy: 0.6100\nEpoch 13/24 - Validation loss: 1.1745, Accuracy: 0.5629\nEpoch 14/24 - Training loss: 1.0124, Accuracy: 0.6215\nEpoch 14/24 - Validation loss: 1.2250, Accuracy: 0.5527\nEpoch 15/24 - Training loss: 0.9878, Accuracy: 0.6284\nEpoch 15/24 - Validation loss: 1.1379, Accuracy: 0.5769\nEpoch 16/24 - Training loss: 0.9684, Accuracy: 0.6336\nEpoch 16/24 - Validation loss: 1.1233, Accuracy: 0.5737\nEpoch 17/24 - Training loss: 0.9435, Accuracy: 0.6458\nEpoch 17/24 - Validation loss: 1.1574, Accuracy: 0.5745\nEpoch 18/24 - Training loss: 0.9267, Accuracy: 0.6520\nEpoch 18/24 - Validation loss: 1.1410, Accuracy: 0.5717\nEpoch 19/24 - Training loss: 0.9123, Accuracy: 0.6586\nEpoch 19/24 - Validation loss: 1.1808, Accuracy: 0.5773\nEpoch 20/24 - Training loss: 0.8923, Accuracy: 0.6672\nEpoch 20/24 - Validation loss: 1.1655, Accuracy: 0.5896\nEpoch 21/24 - Training loss: 0.8726, Accuracy: 0.6747\nEpoch 21/24 - Validation loss: 1.1756, Accuracy: 0.5772\nEpoch 22/24 - Training loss: 0.8587, Accuracy: 0.6800\nEpoch 22/24 - Validation loss: 1.1422, Accuracy: 0.5805\nEpoch 23/24 - Training loss: 0.8394, Accuracy: 0.6839\nEpoch 23/24 - Validation loss: 1.1573, Accuracy: 0.5810\nEpoch 24/24 - Training loss: 0.8156, Accuracy: 0.6960\nEpoch 24/24 - Validation loss: 1.2213, Accuracy: 0.5618\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\ntest_corrects = 0\n\n# You should define nb_test_samples before this block\n# It should be the total number of samples in the test set\nnb_test_samples = len(test_dataloader.dataset)\n\nwith torch.no_grad():\n    for inputs, labels in test_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item() * inputs.size(0)\n\n        _, preds = torch.max(outputs, 1)\n        test_corrects += torch.sum(preds == labels.data).item()\n\n# Calculate the average loss and accuracy\ntest_epoch_loss = test_loss / nb_test_samples\ntest_epoch_accuracy = test_corrects / nb_test_samples\ntest_accuracy_percentage = test_epoch_accuracy * 100\n\nprint(f'Test loss: {test_epoch_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy_percentage:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:04:40.198143Z","iopub.execute_input":"2024-04-07T04:04:40.198538Z","iopub.status.idle":"2024-04-07T04:05:20.668947Z","shell.execute_reply.started":"2024-04-07T04:04:40.198503Z","shell.execute_reply":"2024-04-07T04:05:20.667924Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Test loss: 1.2333\nTest Accuracy: 56.62%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}